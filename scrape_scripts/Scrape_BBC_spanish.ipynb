{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import feedparser\n",
    "import os\n",
    "import numpy as np\n",
    "from requests import Session\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 977 ms, total: 14.5 s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/mundo/topics/c7zp57yyz25t/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    time.sleep(2)   \n",
    "    soup=BeautifulSoup(page.content,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data = data.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data[\"category\"] = \"americana latina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 15 s, sys: 1.46 s, total: 16.5 s\n",
      "Wall time: 10min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data2 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://web.archive.org/web/20200826025820/https://www.bbc.com/mundo/topics/c7zp57yyz25t/page/'+str(page) #august 2020\n",
    "    print('processing page :', page)\n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    soup=BeautifulSoup(page.content,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data2 = data2.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"category\"] = \"americana latina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['url'] = data2['url'].str.replace(r'https://www\\.bbc\\.com/web/20210420210403/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bbc.com/mundo/noticias-america-latina-56813026'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_latina = data.append(data2, ignore_index=True)\n",
    "data_latina = data_latina.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 12.7 s, sys: 883 ms, total: 13.6 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data3 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/mundo/topics/c2lej05epw5t/page/'+str(page)\n",
    "    print('processing page :', page)\n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    soup=BeautifulSoup(page.content,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data3 = data3.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[\"category\"] = \"internacional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 13.4 s, sys: 1.6 s, total: 15 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data4 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/mundo/topics/c06gq9v4xp3t/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.   \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data4 = data4.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4[\"category\"] = \"economia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 13.2 s, sys: 1.6 s, total: 14.8 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data5 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/mundo/topics/ckdxnw959n7t/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data5 = data5.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5[\"category\"] = \"ciencia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 13.4 s, sys: 1.59 s, total: 14.9 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data6 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/mundo/topics/cpzd498zkxgt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data6 = data6.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6[\"category\"] = \"salud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 1.53 s, total: 14.9 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100    \n",
    "\n",
    "data7 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/mundo/topics/c2dwq9zyv4yt/page/'+str(page)\n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data7 = data7.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7[\"category\"] = \"sociedad y cultura\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "CPU times: user 12.6 s, sys: 1.51 s, total: 14.1 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 94    \n",
    "\n",
    "data8 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/mundo/topics/cyx5krnw38vt/page/'+str(page)\n",
    "    print('processing page :', page)\n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data8 = data8.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8[\"category\"] = \"tecnologia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_latina.append([data3,data4,data5,data6,data7,data8], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7891, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.drop_duplicates(subset='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5988, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 32s, sys: 39.8 s, total: 5min 12s\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = Session()\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\\\n",
    "                         'AppleWebKit/537.36 (KHTML, like Gecko) '\\\n",
    "                         'Chrome/75.0.3770.80 Safari/537.36'}\n",
    "# Add headers\n",
    "s.headers.update(headers)\n",
    "\n",
    "    \n",
    "def get_user_need(page):\n",
    "    try:\n",
    "        article = s.get(page)\n",
    "        soup = BeautifulSoup(article.content, 'lxml')\n",
    "        x = [i for i in soup.find(\"script\",text = re.compile(r'(?<=\"campaignName\".\").+?(?=\\\")'))][0]\n",
    "        return re.search(r'(?<=\"campaignName\".\").+?(?=\\\")',x)[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def get_text(url):\n",
    "    try:\n",
    "        article = s.get(url)\n",
    "        soup = BeautifulSoup(article.content, 'lxml')\n",
    "        body = soup.find(role='main')\n",
    "        text = [p.text for p in body.find_all('p',dir=\"ltr\")]\n",
    "        return ''.join(text)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "pool = ThreadPool(4)\n",
    "user_need = pool.map(get_user_need, data_all[\"url\"])\n",
    "data_all[\"User Need\"] = user_need\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 1min 9s, total: 5min 4s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "page_text = pool.map(get_text, data_all[\"url\"])\n",
    "data_all[\"text\"] = page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all[\"User Need\"] = data_all[\"User Need\"].apply(lambda s: re.sub(r\"(WS - )\", \"\", s) if pd.notnull(s) else s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.dropna(subset=[\"User Need\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Give me perspective      1699\n",
       "Educate me               1135\n",
       "Update me                1085\n",
       "Keep me on trend          515\n",
       "Inspire me                268\n",
       "CS                        219\n",
       "Divert me                  96\n",
       "News - Money and Work       2\n",
       "Name: User Need, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all[\"User Need\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5019, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv(\"BBC_user_needs_sp.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

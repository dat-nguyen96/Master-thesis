{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from requests import Session\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Session()\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\\\n",
    "                         'AppleWebKit/537.36 (KHTML, like Gecko) '\\\n",
    "                         'Chrome/75.0.3770.80 Safari/537.36'}\n",
    "# Add headers\n",
    "s.headers.update(headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_items(category,pages):\n",
    "  pagesToGet = pages\n",
    "  data = pd.DataFrame([])\n",
    "  for page in range(1,pagesToGet+1):\n",
    "      url = category + '/page/' + str(page)\n",
    "      #an exception might be thrown, so the code should be in a try-except block\n",
    "      try:\n",
    "          #use the browser to get the url. This is suspicious command that might blow up.\n",
    "          page=s.get(url)                             # this might throw an exception if something goes wrong.\n",
    "      except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "          error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "          print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "          print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "          continue                                              #ignore this page. Abandon this and go back. \n",
    "      soup=BeautifulSoup(page.text,'lxml')\n",
    "      links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "      for i in links:\n",
    "          try: \n",
    "              title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "              link = \"https://www.bbc.com\"\n",
    "              link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "              date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "              data = data.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "          except:\n",
    "              continue\n",
    "  return data\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR FOR LINK: https://www.bbc.com/vietnamese/topics/ckdxnx1x5rnt/page/1\n",
      "<class 'NameError'> Line: 9\n",
      "CPU times: user 1.58 ms, sys: 95 Âµs, total: 1.67 ms\n",
      "Wall time: 1.59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "page1 = get_article_items(\"https://www.bbc.com/vietnamese/topics/ckdxnx1x5rnt\",1) #url = category url, pages = amount of pages available on category page\n",
    "page1[\"category\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_need(page):\n",
    "    try:\n",
    "        article = s.get(page)\n",
    "        soup = BeautifulSoup(article.content, 'lxml')\n",
    "        x = [i for i in soup.find(\"script\",text = re.compile(r'(?<=\"campaignName\".\").+?(?=\\\")'))][0]\n",
    "        return re.search(r'(?<=\"campaignName\".\").+?(?=\\\")',x)[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def get_text(url):\n",
    "    try:\n",
    "        article = s.get(url)\n",
    "        soup = BeautifulSoup(article.content, 'lxml')\n",
    "        body = soup.find(role='main')\n",
    "        text = [p.text for p in body.find_all('p',dir=\"ltr\")]\n",
    "        return ''.join(text)\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    futures2 = []\n",
    "    for page in df[\"url\"]:\n",
    "        futures.append(executor.submit(get_user_need, page))\n",
    "        futures2.append(executor.submit(get_text, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userneed = [df.result() for df in futures]\n",
    "df[\"User Need\"] = userneed\n",
    "df[\"User Need\"] = df[\"User Need\"].apply(lambda s: re.sub(r\"(WS - )\", \"\", s) if pd.notnull(s) else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [df.result() for df in futures2]\n",
    "df[\"text\"] = text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import feedparser\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 14.3 s, sys: 408 ms, total: 14.7 s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/portuguese/topics/cz74k717pw5t/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    time.sleep(2)   \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data = data.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"category\"] = \"brasil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 13.9 s, sys: 340 ms, total: 14.3 s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data2 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/portuguese/topics/cmdm4ynm24kt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    time.sleep(2)   \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data2 = data2.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"category\"] = \"internacional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "CPU times: user 8.66 s, sys: 196 ms, total: 8.85 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 97\n",
    "\n",
    "data3 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/portuguese/topics/cvjp2jr0k9rt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data3 = data3.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[\"category\"] = \"economia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "processing page : 2\n",
      "processing page : 3\n",
      "processing page : 4\n",
      "processing page : 5\n",
      "processing page : 6\n",
      "processing page : 7\n",
      "processing page : 8\n",
      "processing page : 9\n",
      "processing page : 10\n",
      "processing page : 11\n",
      "processing page : 12\n",
      "processing page : 13\n",
      "processing page : 14\n",
      "processing page : 15\n",
      "processing page : 16\n",
      "processing page : 17\n",
      "processing page : 18\n",
      "processing page : 19\n",
      "processing page : 20\n",
      "processing page : 21\n",
      "processing page : 22\n",
      "processing page : 23\n",
      "processing page : 24\n",
      "processing page : 25\n",
      "processing page : 26\n",
      "processing page : 27\n",
      "processing page : 28\n",
      "processing page : 29\n",
      "processing page : 30\n",
      "processing page : 31\n",
      "processing page : 32\n",
      "processing page : 33\n",
      "processing page : 34\n",
      "processing page : 35\n",
      "processing page : 36\n",
      "processing page : 37\n",
      "processing page : 38\n",
      "processing page : 39\n",
      "processing page : 40\n",
      "processing page : 41\n",
      "processing page : 42\n",
      "processing page : 43\n",
      "processing page : 44\n",
      "processing page : 45\n",
      "processing page : 46\n",
      "processing page : 47\n",
      "processing page : 48\n",
      "processing page : 49\n",
      "processing page : 50\n",
      "processing page : 51\n",
      "processing page : 52\n",
      "processing page : 53\n",
      "processing page : 54\n",
      "processing page : 55\n",
      "processing page : 56\n",
      "processing page : 57\n",
      "processing page : 58\n",
      "processing page : 59\n",
      "processing page : 60\n",
      "processing page : 61\n",
      "processing page : 62\n",
      "processing page : 63\n",
      "processing page : 64\n",
      "processing page : 65\n",
      "processing page : 66\n",
      "processing page : 67\n",
      "processing page : 68\n",
      "processing page : 69\n",
      "processing page : 70\n",
      "processing page : 71\n",
      "processing page : 72\n",
      "processing page : 73\n",
      "processing page : 74\n",
      "processing page : 75\n",
      "processing page : 76\n",
      "processing page : 77\n",
      "processing page : 78\n",
      "processing page : 79\n",
      "processing page : 80\n",
      "processing page : 81\n",
      "processing page : 82\n",
      "processing page : 83\n",
      "processing page : 84\n",
      "processing page : 85\n",
      "processing page : 86\n",
      "processing page : 87\n",
      "processing page : 88\n",
      "processing page : 89\n",
      "processing page : 90\n",
      "processing page : 91\n",
      "processing page : 92\n",
      "processing page : 93\n",
      "processing page : 94\n",
      "processing page : 95\n",
      "processing page : 96\n",
      "processing page : 97\n",
      "processing page : 98\n",
      "processing page : 99\n",
      "processing page : 100\n",
      "CPU times: user 12.8 s, sys: 336 ms, total: 13.1 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100\n",
    "\n",
    "data4 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.com/portuguese/topics/c340q430z4vt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data4 = data4.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4[\"category\"] = \"saúde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 297 ms, total: 13.2 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 100    \n",
    "\n",
    "data5 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/portuguese/topics/cr50y580rjxt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data5 = data5.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5[\"category\"] = \"ciência\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso do 1º voo de helicóptero da Nasa em ...</td>\n",
       "      <td>https://www.bbc.com/portuguese/geral-56804874</td>\n",
       "      <td>18:07 19 abril 2021</td>\n",
       "      <td>ciência</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O desafio de se preservar marsupial 'fofo' e m...</td>\n",
       "      <td>https://www.bbc.com/portuguese/vert-tra-56524446</td>\n",
       "      <td>17:25 19 abril 2021</td>\n",
       "      <td>ciência</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A68: o derretimento do maior iceberg do mundo</td>\n",
       "      <td>https://www.bbc.com/portuguese/geral-56796744</td>\n",
       "      <td>13:02 19 abril 2021</td>\n",
       "      <td>ciência</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O fenômeno comovente descoberto por médico que...</td>\n",
       "      <td>https://www.bbc.com/portuguese/geral-56590529</td>\n",
       "      <td>14:40 18 abril 2021</td>\n",
       "      <td>ciência</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Elite poluidora': ricos do mundo precisam red...</td>\n",
       "      <td>https://www.bbc.com/portuguese/internacional-5...</td>\n",
       "      <td>10:58 18 abril 2021</td>\n",
       "      <td>ciência</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  O sucesso do 1º voo de helicóptero da Nasa em ...   \n",
       "1  O desafio de se preservar marsupial 'fofo' e m...   \n",
       "2      A68: o derretimento do maior iceberg do mundo   \n",
       "3  O fenômeno comovente descoberto por médico que...   \n",
       "4  'Elite poluidora': ricos do mundo precisam red...   \n",
       "\n",
       "                                                 url                 date  \\\n",
       "0      https://www.bbc.com/portuguese/geral-56804874  18:07 19 abril 2021   \n",
       "1   https://www.bbc.com/portuguese/vert-tra-56524446  17:25 19 abril 2021   \n",
       "2      https://www.bbc.com/portuguese/geral-56796744  13:02 19 abril 2021   \n",
       "3      https://www.bbc.com/portuguese/geral-56590529  14:40 18 abril 2021   \n",
       "4  https://www.bbc.com/portuguese/internacional-5...  10:58 18 abril 2021   \n",
       "\n",
       "  category  \n",
       "0  ciência  \n",
       "1  ciência  \n",
       "2  ciência  \n",
       "3  ciência  \n",
       "4  ciência  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.54 s, sys: 122 ms, total: 5.66 s\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 42    \n",
    "\n",
    "data6 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/portuguese/topics/c404v027pd4t/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data6 = data6.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6[\"category\"] = \"Tecnologia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Por que gigantes de tecnologia agora rejeitam ...</td>\n",
       "      <td>https://www.bbc.com/portuguese/geral-56794007</td>\n",
       "      <td>18:59 19 abril 2021</td>\n",
       "      <td>Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acidente com carro 'sem motorista' da Tesla ma...</td>\n",
       "      <td>https://www.bbc.com/portuguese/internacional-5...</td>\n",
       "      <td>17:44 19 abril 2021</td>\n",
       "      <td>Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instagram se desculpa por recomendar conteúdo ...</td>\n",
       "      <td>https://www.bbc.com/portuguese/geral-56780392</td>\n",
       "      <td>20:10 16 abril 2021</td>\n",
       "      <td>Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook enfrenta protestos contra plano de cr...</td>\n",
       "      <td>https://www.bbc.com/portuguese/salasocial-5676...</td>\n",
       "      <td>19:11 15 abril 2021</td>\n",
       "      <td>Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Garoto de 14 anos é recrutado para vender crac...</td>\n",
       "      <td>https://www.bbc.com/portuguese/internacional-5...</td>\n",
       "      <td>10:46 14 abril 2021</td>\n",
       "      <td>Tecnologia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Por que gigantes de tecnologia agora rejeitam ...   \n",
       "1  Acidente com carro 'sem motorista' da Tesla ma...   \n",
       "2  Instagram se desculpa por recomendar conteúdo ...   \n",
       "3  Facebook enfrenta protestos contra plano de cr...   \n",
       "4  Garoto de 14 anos é recrutado para vender crac...   \n",
       "\n",
       "                                                 url                 date  \\\n",
       "0      https://www.bbc.com/portuguese/geral-56794007  18:59 19 abril 2021   \n",
       "1  https://www.bbc.com/portuguese/internacional-5...  17:44 19 abril 2021   \n",
       "2      https://www.bbc.com/portuguese/geral-56780392  20:10 16 abril 2021   \n",
       "3  https://www.bbc.com/portuguese/salasocial-5676...  19:11 15 abril 2021   \n",
       "4  https://www.bbc.com/portuguese/internacional-5...  10:46 14 abril 2021   \n",
       "\n",
       "     category  \n",
       "0  Tecnologia  \n",
       "1  Tecnologia  \n",
       "2  Tecnologia  \n",
       "3  Tecnologia  \n",
       "4  Tecnologia  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 12.5 ms, total: 265 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pagesToGet= 2    \n",
    "\n",
    "data7 = pd.DataFrame([])\n",
    "for page in range(1,pagesToGet+1):\n",
    "    url = 'https://www.bbc.com/portuguese/topics/cx6pxx22x5pt/page/'+str(page)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.  \n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    links = soup.find_all('li',attrs={\"class\":\"lx-stream__post-container\"})\n",
    "    for i in links:\n",
    "        try: \n",
    "            title = i.find(\"span\",attrs={'class':'lx-stream-post__header-text gs-u-align-middle'}).text.strip()\n",
    "            link = \"https://www.bbc.com\"\n",
    "            link += i.find('a',attrs={\"class\":\"qa-heading-link lx-stream-post__header-link\"})[\"href\"]\n",
    "            date = i.find('div',attrs={\"class\":\"gs-u-display-flex\"}).find(\"span\",attrs={\"class\":\"qa-post-auto-meta\"}).text\n",
    "            data7 = data7.append(pd.DataFrame({'title': title, 'url': link,'date' : date}, index=[0]), ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7[\"category\"] = \"sala social\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data.append([data2,data3,data4,data5,data6,data7], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5393, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.drop_duplicates(subset='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3517, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_userneed(x):\n",
    "    page=requests.get(x)\n",
    "    soup=BeautifulSoup(page.text,'lxml')\n",
    "    for i in soup.findAll(\"script\"):\n",
    "        try:\n",
    "            return re.search(r'(?<=\"campaignName\".\").+?(?=\\\")', str(i.string))[0]\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    try:\n",
    "        article = requests.get(url)\n",
    "        soup = BeautifulSoup(article.content, 'lxml')\n",
    "        body = soup.find(role='main')\n",
    "        text = [p.text for p in body.find_all('p',dir=\"ltr\")]\n",
    "        return ''.join(text)\n",
    "    except:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 27s, sys: 6.28 s, total: 4min 34s\n",
      "Wall time: 32min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_all[\"User Need\"] = data_all[\"url\"].apply(lambda x : extract_userneed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all[\"User Need\"] = data_all[\"User Need\"].apply(lambda s: re.sub(r\"(WS - )\", \"\", s) if pd.notnull(s) else s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 12s, sys: 4.56 s, total: 3min 16s\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_all[\"text\"] = data_all[\"url\"].apply(lambda x : get_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.dropna(subset=[\"User Need\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Give me perspective    1262\n",
       "Update me               768\n",
       "Keep me on trend        648\n",
       "Educate me              540\n",
       "Inspire me              142\n",
       "CS                      121\n",
       "Divert me                29\n",
       "News - Disclosure         6\n",
       "News - Learning           1\n",
       "Name: User Need, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all[\"User Need\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3517, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.dropna(subset=[\"User Need\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3517, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv(\"brazil_userneeds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
